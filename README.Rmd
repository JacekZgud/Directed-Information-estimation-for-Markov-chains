---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# Markov.dir.info

<!-- badges: start -->
<!-- badges: end -->

The goal of this collection is to simulate the specific class of Markov chains and calculate relevant information theory-based measures. The code is oriented around `MarkovProcess` S4 class object.

## Installation

You can install the development version of `Markov.dir.info` like so:

``` r
install_github("JacekZgud/Directed-Information-estimation-for-Markov-chains")
```

## Example

This is a basic example which shows you how to use the library.

### Initializing `MarkovProcess` class:

```{r definition example,message=FALSE}
library(Markov.dir.info)

# parametrise the code
node_dim = 3
nodes = 3
work_names = tail(LETTERS, nodes)

# define parent structure
ParentStructure = matrix(nrow = nodes, ncol = nodes, data = 0)
rownames(ParentStructure) = colnames(ParentStructure) = work_names

# define the parent structure of nodes
diag(ParentStructure) = 1
ParentStructure[2, 3] = 1
ParentStructure[3, 2] = 1
ParentStructure[1, 2] = 1
ParentStructure[2, 1] = 1

#------------------------------------------------------------------------
# initialize Markov Process class 
process = MarkovProcess(node_dim, nodes, ParentStructure, work_names)
process@trans_prob

# example of quick manual conditional probability changes:

process2 = MarkovProcess(node_dim, nodes, ParentStructure, work_names)
process2@trans_prob$X$prob_0 = c(0.05, 0.05, 0.90, 0.90, 0.05, 0.05, 0.90, 0.90, 0.1)
process2@trans_prob$X$prob_1 = c(0.90, 0.90, 0.05, 0.05, 0.90, 0.90, 0.05, 0.05, 0.1)
process2@trans_prob$X$prob_2 = c(0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.8) 
process2@trans_prob$X


```

### Calculating transition matrix and stationary probability

```{r statio example, message=FALSE}
#------------------------------------------------------------------------
# transition matrix

process = stationary.probability(process)

process = trans.matrix(process)
```

### Calculate transfer entropy

```{r trans entropy example,message=FALSE}

#------------------------------------------------------------------------
#markov process simulation

m = 10 

process = simulate(process, m)
process@simulation

#-------------------------------------------------------------------------
# simulate marginalized markov process
n_2 = 10 

process = simulate.marginalized(process, c('Y', 'Z'), n_2)

process@marg_sim
#calculate transfer entropy from V/target -----> target
process = trans_entropy(process, c('Y', 'Z'), sim.length = n_2)

process@trans_entropy_table
```



## Comparison to Ay Polani information flow using an example from their article.

Script for it may be found below followed by some other visualisations.
```{r}
# example
library(Markov.dir.info)

# parametrise the code
node_dim = 2
nodes = 2
work_names = c("X", "Y")


#define parent structure
ParentStructure = matrix(nrow = nodes, ncol = nodes, data = 0)
rownames(ParentStructure) = colnames(ParentStructure) = work_names
ParentStructure[2, 1] = 1
ParentStructure[1, 1] = 1

prob = seq(from = 1 / 2,
           to = 0.99999,
           length.out =20)


# define function calculating transfer_entropy estimator

info_estimator = function(a, par_struct, n_2 = 1000) {
  proc = MarkovProcess(node_dim, nodes, par_struct, work_names)
  proc@trans_prob$X$prob_1 = c(1 - a, a)
  proc@trans_prob$X$prob_0 = 1 - proc@trans_prob$X$prob_1

  proc@trans_prob$Y$prob_1 = c(1 - a, a)
  proc@trans_prob$Y$prob_0 = 1 - proc@trans_prob$Y$prob_1

  proc = suppressMessages(trans_entropy(proc, c('Y'),n_2))

  return(proc@trans_entropy)
}

#info_estimator(1 / 2, ParentStructure)
infos = rep(0,length(prob))
n = 10
for (i in c(1:n)) {
  infos = infos + Vectorize(info_estimator, 'a')(a = prob, ParentStructure)
}
infos = infos / n

# plot the results

h = function(b) {
  1 - b * log(b / (1 / 2), base = 2) - (1 - b) * log((1 - b) / (1 / 2), base =
                                                       2)
}

mutual_info = function(x) {
  a = x
  b = x
  c = a * (b ^ 2) + a * ((1 - b) ^ 2) + (1 - a) * (b ^ 2) + 2 * (1 - a) *
    b * (1 - b)
  c = 1 - ((1 - a) * b ^ 2 + (1 - a) * (1 - b) ^ 2 + 2 * a * b * (1 - b))
  h(c) - h(x)
}

dir_pol = function(x) {
  1 - h(x)
}


plot(
  prob,
  infos,
  ylim = c(0, 1),
  type = 'line',
  col = 'green',
  xlab = 'a',
  ylab = 'Information'
)
legend(
  "topleft",
  legend = c("Transfer entropy",
             'M.Information|(t-1)',
             'Information Flow'),
  pch = "|",
  col = c("green", "red", 'blue')
)
curve(
  dir_pol,
  from = 0.5,
  to = 1,
  n = 1000,
  add = TRUE,
  col = 'blue'
)
curve(
  mutual_info,
  from = 0.5,
  to = 1.00000000,
  n = 10000,
  add = TRUE,
  col = 'red'
)

```

